{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrdYFGlTOqFQ"
   },
   "source": [
    "# NLP: Character Generation of the Next Chainsmokers Song Using RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDidfu8BOqFS"
   },
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVEnT_mDOqFT"
   },
   "outputs": [],
   "source": [
    "# We'll need to import OS, Time, Tensorflow, and Numpy\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Make sure Tensorflow is version 2.0.0 or higher\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load the path to the file\n",
    "path = \"chainsmokers.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OX8RCLRiOqFX"
   },
   "source": [
    "We want to read in the data and see how many unique characters there are just to get a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AIJ2ILmaOqFY",
    "outputId": "391b1def-a41b-4e5a-fbc1-bee686c4b97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64 unique characters\n"
     ]
    }
   ],
   "source": [
    "# the encoding will help the file recognize that it's ascii english\n",
    "file = open(path, \"rb\").read().decode(encoding='utf-8')\n",
    "vocab = sorted(set(file))\n",
    "print(\"There are \" + str(len(vocab)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbUh7n8UOqFc"
   },
   "source": [
    "## Vectorizing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzK0pP1_OqFd"
   },
   "source": [
    "The first step before training this NLP program is to parse and map the strings in tables. We will use one table to map characters to numbers and another for numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zSxNDmwROqFe"
   },
   "outputs": [],
   "source": [
    "# create the mapping\n",
    "ch_to_int = {a:i for i, a in enumerate(vocab)}\n",
    "    \n",
    "int_to_ch = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([ch_to_int[c] for c in file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gw2Wh9wQOqFh"
   },
   "source": [
    "## Create training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sFvMd06OqFi"
   },
   "source": [
    "Now we have to divide the text into \"example\" sequences (one row of a dataset) that the model will need to predict. Each input sequence will contain 'max_sentence_len' characters from the text.\n",
    "\n",
    "For instance, if we have a text of \"closer\" and the sequence length is 4, the input sequence will be \"clos\", and the target sequence will be the same sequence length except shifted one character to the right: \"lose\".\n",
    "\n",
    "Some Useful Terms:\n",
    "\n",
    "* An Epoch is when the entire dataset is passed through once.\n",
    "\n",
    "* A sequence is a set of data with a defined/specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "max_sentence_len = 125\n",
    "examples_per_epoch = len(file)//(max_sentence_len+1)\n",
    "\n",
    "# We call upon Tensorflow to finally create our training examples \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# The batch function converts individual characters into sequences of our specified size\n",
    "sequences = char_dataset.batch(max_sentence_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "# For each sequence, use the the `map` method to quickly apply the function to each batch:\n",
    "\n",
    "# The split_target method is simply duplicating and shifting the input and target texts\n",
    "def split_target(text_data):\n",
    "    input_text = text_data[:-1]\n",
    "    target_text = text_data[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "0eBu9WZG84i0",
    "outputId": "8b3c7ebf-7739-4e55-e0ac-7ce42070e796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  Hey, I was doing just fine before I met you\n",
      "I drink too much and that's an issue but I'm okay\n",
      "Hey, you tell your friends it w\n",
      "Target data:  ey, I was doing just fine before I met you\n",
      "I drink too much and that's an issue but I'm okay\n",
      "Hey, you tell your friends it wa\n",
      "Character #0\n",
      "Input char: tf.Tensor(22, shape=(), dtype=int64)H\n",
      "Expected output char: tf.Tensor(42, shape=(), dtype=int64)e\n",
      "Character #1\n",
      "Input char: tf.Tensor(42, shape=(), dtype=int64)e\n",
      "Expected output char: tf.Tensor(62, shape=(), dtype=int64)y\n",
      "Character #2\n",
      "Input char: tf.Tensor(62, shape=(), dtype=int64)y\n",
      "Expected output char: tf.Tensor(7, shape=(), dtype=int64),\n",
      "Character #3\n",
      "Input char: tf.Tensor(7, shape=(), dtype=int64),\n",
      "Expected output char: tf.Tensor(1, shape=(), dtype=int64) \n",
      "Character #4\n",
      "Input char: tf.Tensor(1, shape=(), dtype=int64) \n",
      "Expected output char: tf.Tensor(23, shape=(), dtype=int64)I\n",
      "Character #5\n",
      "Input char: tf.Tensor(23, shape=(), dtype=int64)I\n",
      "Expected output char: tf.Tensor(1, shape=(), dtype=int64) \n",
      "Character #6\n",
      "Input char: tf.Tensor(1, shape=(), dtype=int64) \n",
      "Expected output char: tf.Tensor(60, shape=(), dtype=int64)w\n",
      "Character #7\n",
      "Input char: tf.Tensor(60, shape=(), dtype=int64)w\n",
      "Expected output char: tf.Tensor(38, shape=(), dtype=int64)a\n",
      "Character #8\n",
      "Input char: tf.Tensor(38, shape=(), dtype=int64)a\n",
      "Expected output char: tf.Tensor(56, shape=(), dtype=int64)s\n",
      "Character #9\n",
      "Input char: tf.Tensor(56, shape=(), dtype=int64)s\n",
      "Expected output char: tf.Tensor(1, shape=(), dtype=int64) \n"
     ]
    }
   ],
   "source": [
    "# To verify our training predictor is doing its job so far, let's take a look at the \n",
    "# first sequence and 10 characters of the file \n",
    "for input_example, target_example in  dataset.take(1):\n",
    "    print (\"Input data: \", str(''.join(int_to_ch[input_example.numpy()])))\n",
    "    print (\"Target data: \", str(''.join(int_to_ch[target_example.numpy()])))\n",
    "    \n",
    "# Zip helps the enumerate function understand that it's looking at an integer array\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:10], target_example[:10])):\n",
    "    print(\"Character #\" + str(i))\n",
    "    print(\"Input char: \" + str(input_idx) + str(int_to_ch[input_idx]))\n",
    "    print(\"Expected output char: \" + str(target_idx) + str(int_to_ch[target_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzCcNkJsOqFt"
   },
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx5YVyc9OqFt"
   },
   "source": [
    "A batch is a set of examples (one row of a dataset) used in one iteration (a single update of a model's weights during training.) of model training. A batch size is the number of examples in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X32gPxkMOqFu",
    "outputId": "74d02a4a-b61d-45b9-e89a-6fa350201fc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 125), (64, 125)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size is necessary to shuffle the data into batches\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlrFlJCzOqFx"
   },
   "source": [
    "At this point in the program, we will be dealing with Tensorflow's built in functions and terminology, which includes using Keras' API. This will greatly reduce our time in trying to code out the math behind these algorithms. Most importantly, we can focus on the general structure of how we go from deciding what model to use to generating the output.\n",
    "\n",
    "But first, we need to go over a bit more terminology to understand what's going on.\n",
    "\n",
    "* Tensor: The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values.\n",
    "\n",
    "* Embedding: A categorical feature represented as a continuous-valued feature. In other words, a several-hundred-element tensor in which each element holds a floating-point value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MH8cIev7OqFy"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoBfxNySOqFz"
   },
   "source": [
    "We will be using a simple sequential model: \n",
    " * 1 layer of input (tf.keras.layers.Embedding) \n",
    " * 1 hidden (tf.keras.layers.GRU) \n",
    " * 1 output (tf.keras.layers.Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BBU-Gu2gOqF0"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units/neurons\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T54Di3iiTFWl"
   },
   "source": [
    "As good coding habit, we need to verify that the model behaves as expected before we decide to spend hours and GPU power to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "88nw6kflOqF3",
    "outputId": "ea8127d5-d4e4-488c-cdd1-23c379ea3e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 125, 64) # (batch_size, sequence_length, vocab_size)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           16384     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 64)            65600     \n",
      "=================================================================\n",
      "Total params: 4,020,288\n",
      "Trainable params: 4,020,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Input: \n",
      " \"ng you up\\nCome on and give me some love tonight\\nOoh, you're all that I want\\nNo good at giving you up\\nCome on and give me some\"\n",
      "\n",
      "Next Char Predictions: \n",
      " 'AD DYLq-ijd.R)q?8CVRFnM-Lw.LR\\nKMJYzs\\'pAGh!Uun1KgNEqAddFoA!a\"hc8BpGLlvJl-rzBS-2U8Y4TuwYw?ElT\\nsR1Lz\\n AmyLWKx\"!VqP4xUuM\\nuDY8nkr\\n'\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(int_to_ch[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(int_to_ch[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LfmZAwVOqF5"
   },
   "source": [
    "## Optimize and Loss Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xTsPcnqOqF6"
   },
   "source": [
    "* Loss: A measure of how far a model's predictions are from its label - a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss. tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions. The smaller this number is, the better.\n",
    "\n",
    "* Adam optimizer: Stands for ADAptive with Momentum. Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "\n",
    "* Logits: Vector of raw predictions and probabilities, which will then be used to calculate the normalized probability. \n",
    "\n",
    "* Label: The \"answer\" we're trying to train the model to achieve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-km2bNOyOqF7",
    "outputId": "9f26a0e9-dfb6-47af-bcd6-05e2d8f8f26b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 125, 64)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:       4.158063\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:      \", example_batch_loss.numpy().mean())\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eI--qaJHOqF-"
   },
   "source": [
    "## Set up the checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeeqSwH4OqF-"
   },
   "source": [
    "Checkpoints enable exporting model weights, as well as performing training across multiple sessions. Checkpoints also enable training to continue past errors (for example, job preemption). Note that the graph itself is not included in a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xblnsBYHOqGA"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qo2EOG6EOqGC"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lIibbsxaOqGD"
   },
   "source": [
    "Now comes the training piece: we call Tensorflow's fit() function to make our model \"fit\" which basically adjusts the model parameters to minimize the loss.\n",
    "\n",
    "This is now just a standard classification problem. Classification models are a type of machine learning model for distinguishing among two or more classes. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jOuXSOuUOqGE",
    "outputId": "cbad1e95-5ec0-4a51-9c5f-e8138e3fe6d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "4/4 [==============================] - 34s 9s/step - loss: 4.0145\n",
      "Epoch 2/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 5.6518\n",
      "Epoch 3/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 3.9499\n",
      "Epoch 4/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 3.9001\n",
      "Epoch 5/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 3.7525\n",
      "Epoch 6/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 3.4706\n",
      "Epoch 7/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 3.1902\n",
      "Epoch 8/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 3.0202\n",
      "Epoch 9/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.9624\n",
      "Epoch 10/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 2.8641\n",
      "Epoch 11/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 2.7831\n",
      "Epoch 12/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.7010\n",
      "Epoch 13/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.6175\n",
      "Epoch 14/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.5398\n",
      "Epoch 15/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.4792\n",
      "Epoch 16/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.4317\n",
      "Epoch 17/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.3796\n",
      "Epoch 18/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.3215\n",
      "Epoch 19/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 2.2866\n",
      "Epoch 20/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.2400\n",
      "Epoch 21/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.2087\n",
      "Epoch 22/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 2.1791\n",
      "Epoch 23/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.1431\n",
      "Epoch 24/80\n",
      "4/4 [==============================] - 34s 9s/step - loss: 2.1304\n",
      "Epoch 25/80\n",
      "4/4 [==============================] - 34s 9s/step - loss: 2.0969\n",
      "Epoch 26/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 2.0663\n",
      "Epoch 27/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 2.0314\n",
      "Epoch 28/80\n",
      "4/4 [==============================] - 34s 9s/step - loss: 2.0183\n",
      "Epoch 29/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.9987\n",
      "Epoch 30/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.9378\n",
      "Epoch 31/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.9164\n",
      "Epoch 32/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.9132\n",
      "Epoch 33/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.8775\n",
      "Epoch 34/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.8389\n",
      "Epoch 35/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.8153\n",
      "Epoch 36/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.7983\n",
      "Epoch 37/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.7496\n",
      "Epoch 38/80\n",
      "4/4 [==============================] - 34s 9s/step - loss: 1.7328\n",
      "Epoch 39/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.7104\n",
      "Epoch 40/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.6516\n",
      "Epoch 41/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.6278\n",
      "Epoch 42/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.6041\n",
      "Epoch 43/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.5568\n",
      "Epoch 44/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.5238\n",
      "Epoch 45/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.4950\n",
      "Epoch 46/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.4560\n",
      "Epoch 47/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 1.4162\n",
      "Epoch 48/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.3818\n",
      "Epoch 49/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.3451\n",
      "Epoch 50/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.2734\n",
      "Epoch 51/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.2693\n",
      "Epoch 52/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.2058\n",
      "Epoch 53/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.1908\n",
      "Epoch 54/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.1395\n",
      "Epoch 55/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 1.0986\n",
      "Epoch 56/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.0967\n",
      "Epoch 57/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 1.0370\n",
      "Epoch 58/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.9786\n",
      "Epoch 59/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.9674\n",
      "Epoch 60/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.9124\n",
      "Epoch 61/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.8782\n",
      "Epoch 62/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.8507\n",
      "Epoch 63/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.8112\n",
      "Epoch 64/80\n",
      "4/4 [==============================] - 34s 8s/step - loss: 0.7839\n",
      "Epoch 65/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.7526\n",
      "Epoch 66/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.7207\n",
      "Epoch 67/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.6989\n",
      "Epoch 68/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.6534\n",
      "Epoch 69/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.6532\n",
      "Epoch 70/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.6088\n",
      "Epoch 71/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.5611\n",
      "Epoch 72/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.5635\n",
      "Epoch 73/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.5327\n",
      "Epoch 74/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.4911\n",
      "Epoch 75/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.4887\n",
      "Epoch 76/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.4667\n",
      "Epoch 77/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.4388\n",
      "Epoch 78/80\n",
      "4/4 [==============================] - 33s 8s/step - loss: 0.4221\n",
      "Epoch 79/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.3960\n",
      "Epoch 80/80\n",
      "4/4 [==============================] - 32s 8s/step - loss: 0.3996\n"
     ]
    }
   ],
   "source": [
    "# Increase the EPOCHS for better results\n",
    "EPOCHS=80\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kc7BTVY-OqGI"
   },
   "source": [
    "## Restore last checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHmJ-kzsOqGI"
   },
   "source": [
    "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built. We use a size of 1 to keep it fast and simple. Then we need to rebuild the model and restore the weights from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sB9FV-1sOqGJ"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n559Y2GwOqGN"
   },
   "source": [
    "The following code block generates the text:\n",
    "\n",
    "1) We choose the starting string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "2) Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "3) Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "4) The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTy725lPOqGO"
   },
   "source": [
    "## Loop of text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzg-PqOfOqGO"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "    # Number of characters final song will be. For a standard 280ish word song, that'll be around 1700 characters\n",
    "    # Experiment around here\n",
    "    num_generate = 1700\n",
    "\n",
    "    # Vectorize our start string\n",
    "    input_eval = [ch_to_int[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # String to store results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature = predictable text.\n",
    "    # Higher temperature = more surprising text.\n",
    "    temperature = 0.33\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(int_to_ch[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykIvN9zKOqGR"
   },
   "source": [
    "## Create the next banger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "CuulYzAhOqGS",
    "outputId": "cd23681f-3a52-449d-ecac-c460ad29d0e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I punched your dad in my Rover (who?) Bof me I'm like anybody else\n",
      "In the town full of fancy cars and crowded bars and supermodels\n",
      "Looks exactly the way it did inside my head\n",
      "When I dreamed about it\n",
      "All the things I could live without\n",
      "I need it now 'cause they're all around me\n",
      "Only thing that I can't afford is to lose myself\n",
      "Tryna be somebody, somebody\n",
      "Somebody, somebody\n",
      "(You know, just know what I like)\n",
      "Somebody\n",
      "You should just this is all we know\n",
      "'Cause this is all we know\n",
      "I'll break your heart so you don't break mine\n",
      "Before I love you (nah, nah, nah)\n",
      "I'm gonna leave you (nah, nah, nah)\n",
      "I'm gonna leave you (nah, nah, nah)\n",
      "I'm gonna leave you (nah, nah, nah)\n",
      "Even if I'm not here to stay\n",
      "I still want your heart\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway, yeah, yeah, yeah yeah\n",
      "Your heart for takeaway\n"
     ]
    }
   ],
   "source": [
    "#Add your first lyrics by replacing the string with whatver you want\n",
    "print(generate_text(model, start_string=\"I punched your dad in my Rover \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "The easiest thing you can do to improve the results it to train it for longer, so increase the EPOCHS variable. If you can get the loss to below 0.5, you're sitting pretty.\n",
    "\n",
    "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAC9CB9JOqGV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
